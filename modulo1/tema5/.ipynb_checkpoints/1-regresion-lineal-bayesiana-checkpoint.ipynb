{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Lineal Revisitada\n",
    "\n",
    "![BLR](https://upload.wikimedia.org/wikipedia/commons/e/ed/Residuals_for_Linear_Regression_Fit.png)\n",
    "\n",
    "> Anteriormente vimos que el estimador MAP, con previa Normal, para ajustar los parámetros $w\\in\\mathbb{R}^{d}$ de una función lineal $f(x) = x^T w$ a un conjunto de puntos $\\{(x_i, y_i)\\}_{i=1}^{N}$, con $x_i\\in\\mathbb{R}^{d}$ y $y_i\\in\\mathbb{R}$, tiene la forma de mínimos cuadrados regularizados.\n",
    ">\n",
    "> $$\n",
    "  \\hat{w}_{MAP} = \\arg \\min_{w} \\left|\\left|y - \\Phi w\\right|\\right|^2 + \\lambda \\left|\\left|w\\right|\\right|^2.\n",
    "  $$\n",
    "\n",
    "> Hoy usaremos ideas similares, y nos basaremos en la idea de previas conjugadas para estimar **la distribución posterior completa**, y extraer conclusiones adicionales a la estimación de los parámetros más probables.\n",
    "\n",
    "> **Objetivos:**\n",
    "> - Estudiar un enfoque Bayesiano sobre el problema de ajuste de una función lineal a un conjuto de datos.\n",
    "\n",
    "> **Referencias:**\n",
    "> \n",
    "> - Bayesian Methods for Machine Learning course, HSE University, Coursera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Planteamiento inicial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez más, modelamos la relación entre las V.A. objetivo $y$, parámetros $w$, y datos de entrada $x$ de forma lineal, con incertidumbre aditiva Gaussiana:\n",
    "\n",
    "$$\n",
    "y = \\phi(x)^T w + \\epsilon,\n",
    "$$\n",
    "\n",
    "con $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como **no nos interesa modelar los datos, sino los parámetros**, estudiamos la distribución posterior $p(w | y, X)$, la cual se puede reescribir como:\n",
    "\n",
    "\\begin{align}\n",
    "p(w | y, X) = \\frac{p(y | X, w) p(w)}{p(y | X)} \\qquad \\text{(Bayes & $w \\perp X$)}.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La vez pasada, nos evitamos el problema de calcular la distribución posterior usando un estimador MAP para los parámetros, de la siguiente manera:\n",
    "\n",
    "\\begin{align}\n",
    "p(w | y, X)               & \\propto p(y | X, w) p(w) \\qquad \\text{(La evidencia no depende de $w$)} \\\\\n",
    "\\arg \\max_{w} p(w | y, X) & = \\arg \\max_{w} p(y | X, w) p(w).\n",
    "\\end{align}\n",
    "\n",
    "Sin embargo, comentamos que aunque el estimador MAP posee características interesantes, sigue siendo un estimador puntual, lo que puede no ser suficiente dependiendo de la aplicación.\n",
    "\n",
    "Durante esta sesión, estaremos modelando la distribución posterior completa, usando la idea de **distribuciones previas conjugadas**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Previas conjugadas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como antes, dado nuestor modelo, la función de verosimilitud $P(y | X, w)$ es una distribución normal de la forma:\n",
    "\n",
    "$$\n",
    "p(y | X, w) = \\mathcal{N}(y | \\Phi w, \\beta^{-1} I) \\propto \\exp\\left\\{-\\frac{\\beta}{2} ||y - \\Phi w||^2 \\right\\},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que, para cualquier $w_0 \\in \\mathbb{R}^d$:\n",
    "\n",
    "\\begin{align}\n",
    "||y - \\Phi w||^2 & = ||\\Phi w - y||^2 \\\\\n",
    "                 & = ||\\Phi (w - w_0) + \\Phi w_0 - y||^2 \\\\\n",
    "                 & = \\left(\\Phi (w - w_0) + \\Phi w_0 - y\\right)^T \\left(\\Phi (w - w_0) + \\Phi w_0 - y\\right) \\\\\n",
    "                 & = (w - w_0)^T \\Phi^T \\Phi (w - w_0) + 2 (w - w_0)^T \\Phi^T (\\Phi w_0 - y) + ||\\Phi w_0- y||^2 \n",
    "\\end{align}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En particular, haciendo $w_0 = \\hat{w}_{MLE} = \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y$ (parámetros de máxima verosimilitud), se observa que:\n",
    "\n",
    "\\begin{array}\n",
    "2 (w - \\hat{w}_{MLE})^T \\Phi^T (\\Phi \\hat{w}_{MLE} - y) & = 2 (\\Phi w - \\Phi \\hat{w}_{MLE})^T  (\\Phi \\hat{w}_{MLE} - y) \\\\\n",
    "                                    & = 2 \\left[w^T \\Phi^T \\Phi \\hat{w}_{MLE} - w^T \\Phi^Ty - (\\hat{w}_{MLE})^T \\Phi^T \\Phi \\hat{w}_{MLE} + (\\hat{w}_{MLE})^T \\Phi^T y \\right]\\\\\n",
    "                                    & = 2 \\left[w^T \\underbrace{\\Phi^T \\Phi \\left(\\Phi^T \\Phi\\right)^{-1}}_{I} \\Phi^T y - w^T \\Phi^Ty - y^T \\Phi \\underbrace{\\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T \\Phi}_{I} \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y + y^T \\Phi \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y\\right] \\\\\n",
    "                                    & = 2 \\left[w^T \\Phi^T y - w^T \\Phi^Ty - y^T \\Phi  \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y + y^T \\Phi \\left(\\Phi^T \\Phi\\right)^{-1} \\Phi^T y\\right] \\\\\n",
    "                                    & = 0.\n",
    "\\end{array}\n",
    "\n",
    "De esta manera: \n",
    "\n",
    "$$\n",
    "||y - \\Phi w||^2 = (w - \\hat{w}_{MLE})^T \\Phi^T \\Phi (w - \\hat{w}_{MLE}) + \\underbrace{||\\Phi \\hat{w}_{MLE}- y||^2 }_{f(X, y)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, concluimos que\n",
    "\n",
    "\\begin{align}\n",
    "P(y | X, w) & = \\mathcal{N}(y | X w, \\sigma^2 I) \\\\\n",
    "            & \\propto \\exp\\left\\{-\\frac{\\beta}{2} ||y - \\Phi w||^2\\right\\} \\\\\n",
    "            & \\propto \\exp\\left\\{-\\frac{\\beta}{2} (w - w^*)^T \\Phi^T \\Phi (w - w^*)\\right\\} \\\\\n",
    "            & \\propto \\mathcal{N}\\left(w | w^*, \\beta^{-1} \\left(\\Phi^T \\Phi\\right)^{-1}\\right)\n",
    "\\end{align}\n",
    "\n",
    "#### La verosimilitud es proporcional a una distribución normal sobre los parámetros, con media igual al estimador por máxima verosimilitud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retomando la distribución posterior, sabemos que\n",
    "\n",
    "$$\n",
    "P(w | X, y) \\propto P(y | X, w) P(w) \\propto \\mathcal{N}\\left(w | w^*, \\beta^{-1} \\left(\\Phi^T \\Phi\\right)^{-1}\\right) P(w).\n",
    "$$\n",
    "\n",
    "¿Qué elecciones de la distribución previa $P(w)$ son conjugadas?\n",
    "\n",
    "#### Una distribución normal sobre $w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Previa normal\n",
    "\n",
    "Como vimos en la tarea, el producto de dos densidades normales univariadas es otra densidad normal univariada. Esto se extiende al caso multivariable con:\n",
    "\n",
    "$$\n",
    "\\mu_3 = \\Sigma_2 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\mu_1 + \\Sigma_1 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\mu_2, \\qquad \\Sigma_3 = \\Sigma_1 \\left(\\Sigma_1 + \\Sigma_2\\right)^{-1} \\Sigma_2.\n",
    "$$\n",
    "\n",
    "De manera que una previa conjugada para el caso anterior es:\n",
    "\n",
    "$$\n",
    "P(w) = \\mathcal{N}(w | 0, \\alpha^{-1} I),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "con lo cual:\n",
    "\n",
    "$$\n",
    "P(w | X, y) = \\mathcal{N}\\left(w | w^{**}, \\Sigma^{*}\\right).\n",
    "$$\n",
    "\n",
    "con\n",
    "\n",
    "$$\n",
    "\\Sigma^{*} = \\beta^{-1} \\left(\\Phi^T \\Phi + \\lambda I \\right)^{-1}, \\qquad w^{**} = \\left(\\Phi^T \\Phi + \\lambda I \\right)^{-1} \\Phi^T y = \\hat{w}_{MAP}, \n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "> **Comentario:** Para todo este análisis hemos supuesto los parámetros $\\beta$ y $\\alpha$ como valores deterministas conocidos. De no ser cierto esto, tendríamos que modelarlos como VA adicionales, y esto aumentaría un poco la complejidad del problema.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejemplo:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar numpy \n",
    "import numpy as np\n",
    "# Importar scipy.stats\n",
    "from scipy import stats\n",
    "# Importar matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "# Importar mpl_toolkits.mplot3d\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fijar semilla para reproducibilidad\n",
    "np.random.seed(1001)\n",
    "# Dispersión de los datos\n",
    "sigma = 2\n",
    "# Generar datos\n",
    "N = 100\n",
    "x = np.linspace(0, 10, N)\n",
    "y = 2 * x + 5 + np.random.normal(loc=0, scale=sigma, size=N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar\n",
    "plt.plot(x, y, 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, tenemos que el modelo es:\n",
    "\n",
    "$$\n",
    "y = \\underbrace{[1\\quad x]}_{\\phi(x)^T} \\left[\\begin{array}{c} w_0 \\\\ w_1\\end{array} \\right] + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construimos Phi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De los datos, un valor plausible para $\\alpha$ es: $\\alpha=1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta y Alpha\n",
    "\n",
    "# Distribución posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "w1 = np.linspace(3.0, 5.3, 100)\n",
    "w2 = np.linspace(1.9, 2.5, 100)\n",
    "w1, w2 = np.meshgrid(w1, w2)\n",
    "z = posterior.pdf(np.dstack([w1, w2]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(w1, w2, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(w1, w2, z)\n",
    "ax2.plot(w_map[0], w_map[1], '*r', ms=10)\n",
    "ax2.plot(5, 2, '*g', ms=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.contour(w1, w2, z)\n",
    "w_samples = posterior.rvs(size=500)\n",
    "ax1.plot(w_samples[:, 0], w_samples[:, 1], 'ob', alpha=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros más probables y diferentes posibilidades\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Qué pasa si el valor que usamos para $\\sigma$ se desvía del valor real?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beta y Alpha\n",
    "\n",
    "# Distribución posterior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gráfico\n",
    "w1 = np.linspace(3.0, 6.3, 100)\n",
    "w2 = np.linspace(1.5, 2.5, 100)\n",
    "w1, w2 = np.meshgrid(w1, w2)\n",
    "z = posterior.pdf(np.dstack([w1, w2]))\n",
    "fig = plt.figure(figsize=(8, 4))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax1.plot_surface(w1, w2, z)\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.contour(w1, w2, z)\n",
    "ax2.plot(w_map[0], w_map[1], '*r', ms=10)\n",
    "ax2.plot(5, 2, '*g', ms=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aprendizaje en línea\n",
    "\n",
    "Este esquema que acabamos de ver es susceptible para aprendizaje en línea. Dado que la distribución posterior es normal cuando la previa es normal (previa conjugada), podemos usar esta distribución posterior para el siguiente paso. La nueva previa para el siguiente paso sería:\n",
    "\n",
    "$$\n",
    "P_{k+1}(w) = P_{k}(w | X, y) = \\mathcal{N}\\left(w | \\mu_k, \\Sigma_k\\right).\n",
    "$$\n",
    "\n",
    "**Tarea.** Dado que la función de verosimilitud satisface:\n",
    "\n",
    "$$\n",
    "P(y | X, w) \\propto \\mathcal{N}\\left(w | w^*, \\sigma^2 \\left(X^T X\\right)^{-1}\\right),\n",
    "$$\n",
    "\n",
    "¿Cuál sería la distribución posterior con una previa normal $P(w)  = \\mathcal{N}\\left(w | \\mu, \\Sigma\\right)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anuncios parroquiales\n",
    "\n",
    "### Examen módulo 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script>\n",
    "  $(document).ready(function(){\n",
    "    $('div.prompt').hide();\n",
    "    $('div.back-to-top').hide();\n",
    "    $('nav#menubar').hide();\n",
    "    $('.breadcrumb').hide();\n",
    "    $('.hidden-print').hide();\n",
    "  });\n",
    "</script>\n",
    "\n",
    "<footer id=\"attribution\" style=\"float:right; color:#808080; background:#fff;\">\n",
    "Created with Jupyter by Esteban Jiménez Rodríguez.\n",
    "</footer>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
